---
id: chunk
title: Chunk Block
description: Split a string into an array of strings based on token count
sidebarTitle: Chunk
---

![Chunk Block Screenshot](./assets/chunk-block.png)

## Overview
<Frame className="block dark:hidden">
  <img src="/block-reference/assets/chunk_light.png" alt="Chunk Block Screenshot" />
</Frame>
<Frame className="hidden dark:block">
  <img src="/block-reference/assets/chunk_dark.png" alt="Chunk Block Screenshot" />
</Frame>

The Chunk Block is used to split a string into an array of strings based on a token count. This is particularly useful for handling large text inputs that exceed token limits in Language Models (LLMs), or for truncating strings to specific token counts.

## Inputs

<ParamField path="input" type="string" required>
  The string to be split into chunks. Non-string inputs will be coerced to strings.
</ParamField>

<ParamField path="model" type="string">
  The AI model to use for tokenization. Only available when "Use Model Input" is enabled in settings.
</ParamField>

## Outputs

<ResponseField name="chunks" type="string[]">
  An array containing all the chunks after splitting the input string.
</ResponseField>

<ResponseField name="first" type="string">
  The first chunk from the chunks array. Useful for truncating text from the start.
</ResponseField>

<ResponseField name="last" type="string">
  The last chunk from the chunks array. Useful for truncating text from the end.
</ResponseField>

<ResponseField name="indexes" type="number[]">
  A list of sequential numbers starting from 1, one for each chunk. Useful for filtering or zipping with the chunks array.
</ResponseField>

<ResponseField name="count" type="number">
  The total number of chunks created.
</ResponseField>

## Editor Settings

<ParamField path="AI Model" type="string" default="gpt-3.5-turbo">
  The model to use for tokenizing the text. Different models may tokenize text differently. Can be overridden by the "model" input if "Use Model Input" is enabled.
</ParamField>

<ParamField path="Use Model Input" type="boolean" default={false}>
  When enabled, adds a "model" input port that can override the "AI Model" setting.
</ParamField>

<ParamField path="Number of tokens per chunk" type="number" default={1024} min={1} max={32768}>
  The target number of tokens for each chunk. The actual chunk sizes may vary slightly to maintain text coherence.
</ParamField>

<ParamField path="Overlap (in %)" type="number" default={0} min={0} max={100}>
  The percentage of overlap between consecutive chunks. For example, with a 50% overlap and 1000 tokens per chunk, each chunk will share approximately 500 tokens with the next chunk. This helps maintain context between chunks.
</ParamField>

## Example: Chunking a Long Text

1. Create a Text Block with a long piece of text.
2. Add a Chunk Block and connect the Text Block to its input.
3. Configure the desired token count and overlap in the settings.
4. Run the flow. The text will be split into chunks based on your settings.

## Error Handling

The Chunk Block will automatically coerce non-string inputs into strings. No other notable error handling behavior.

## FAQ

<AccordionGroup>
  <Accordion title="Why use chunking for LLMs?">
    Chunking is useful to avoid hitting token count limits in LLMs. You can split a long string into multiple chunks, process each chunk separately, and then combine the results.
  </Accordion>

  <Accordion title="How does the overlap feature work?">
    The overlap percentage determines how much text is shared between consecutive chunks. For example, with 1000 tokens per chunk and 50% overlap, each chunk will share approximately 500 tokens with the next chunk. This helps maintain context and coherence between chunks.
  </Accordion>
</AccordionGroup>

## See Also

- [Batching](/user-guide/workflow/batching)
- [Chat Block](/block-reference/ai/chat)
- [Text Block](/block-reference/data/text)
