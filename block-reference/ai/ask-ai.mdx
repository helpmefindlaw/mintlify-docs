---
id: ask-ai
title: Ask AI Block
description: Send messages to over 250 AI models for chat completions
sidebarTitle: Ask AI
---

## Overview

The Ask AI Block allows you to send messages to over 250 different AI models for chat completions. This versatile block provides a wide range of options for AI-powered conversations and text generation tasks.

<Frame className="block dark:hidden">
  <img src="/block-reference/assets/ask_ai_light.png" alt="Ask AI Screenshot (Light Mode)" />
</Frame>
<Frame className="hidden dark:block">
  <img src="/block-reference/assets/ask_ai_dark.png" alt="Ask AI Screenshot (Dark Mode)" />
</Frame>

## Inputs

<ParamField path="systemPrompt" type="string | chat-message">
  The system prompt to send to the model. Optional. Used to provide high-level guidance to the AI model.
</ParamField>

<ParamField path="prompt" type="string | string[] | chat-message | chat-message[]">
  The prompt message or messages to send to the model. Required. Strings will be converted into chat messages of type `user`, with no name.
</ParamField>

<ParamField path="model" type="string">
  The model to use for the chat. Only available when "Use Model Input" is enabled.
</ParamField>

<ParamField path="temperature" type="number">
  What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Only available when "Use Temperature Input" is enabled.
</ParamField>

<ParamField path="top_p" type="number">
  An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Only available when "Use Top P Input" is enabled.
</ParamField>

<ParamField path="useTopP" type="boolean">
  Whether to use top p sampling, or temperature sampling. Only available when "Use Top P Input" is enabled.
</ParamField>

<ParamField path="maxTokens" type="number">
  The maximum number of tokens to generate in the chat completion. Only available when "Use Max Tokens Input" is enabled.
</ParamField>

<ParamField path="stop" type="string">
  A sequence where the API will stop generating further tokens. Only available when "Use Stop Input" is enabled.
</ParamField>

<ParamField path="presencePenalty" type="number">
  Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Only available when "Use Presence Penalty Input" is enabled.
</ParamField>

<ParamField path="frequencyPenalty" type="number">
  Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. Only available when "Use Frequency Penalty Input" is enabled.
</ParamField>

<ParamField path="seed" type="number">
  If specified, OpenAI will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Only available when "Use Seed Input" is enabled.
</ParamField>

## Outputs

<ResponseField name="response" type="string">
  The textual response from the model.
</ResponseField>

<ResponseField name="in-messages" type="chat-message[]">
  All messages sent to the model.
</ResponseField>

<ResponseField name="all-messages" type="chat-message[]">
  All messages, with the response appended.
</ResponseField>

<ResponseField name="responseTokens" type="number">
  The number of tokens in the response from the LLM. For a multi-response, this is the sum.
</ResponseField>

<ResponseField name="cost" type="number">
  The estimated cost of the API call in USD.
</ResponseField>

<ResponseField name="duration" type="number">
  The time taken to complete the request in milliseconds.
</ResponseField>

## Editor Settings

<ParamField path="Model" type="string">
  The AI model to use for responses. Choose from over 250 available models across various providers.
</ParamField>

<ParamField path="Use Prompt Input" type="boolean" default={true}>
  Whether to use the prompt input, or input a prompt directly in the settings.
</ParamField>

<ParamField path="Temperature" type="number" default={0.7} min={0} max={2} step={0.1}>
  Controls randomness in the output. Lower values make the output more focused and deterministic.
</ParamField>

<ParamField path="Top P" type="number" default={1} min={0} max={1} step={0.1}>
  Alternative to temperature sampling. Only tokens comprising the top P probability mass are considered.
</ParamField>

<ParamField path="Use Top P" type="boolean" default={false}>
  Whether to use top p sampling instead of temperature sampling.
</ParamField>

<ParamField path="Max Tokens" type="number" default={8192}>
  The maximum number of tokens to generate in the completion.
</ParamField>

<ParamField path="Stop" type="string">
  A sequence where the API will stop generating further tokens.
</ParamField>

<ParamField path="Presence Penalty" type="number" min={-2} max={2} step={0.1}>
  Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
</ParamField>

<ParamField path="Frequency Penalty" type="number" min={-2} max={2} step={0.1}>
  Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
</ParamField>

<ParamField path="Seed" type="number" step={1}>
  If specified, OpenAI will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result.
</ParamField>

### Advanced

<ParamField path="Custom Max Tokens" type="number">
  Overrides the max number of tokens a model can support. Leave blank for preconfigured token limits.
</ParamField>

<ParamField path="Cache Responses" type="boolean" default={false}>
  If enabled, requests with the same parameters and messages will be cached for immediate responses without an API call.
</ParamField>

<ParamField path="Use for subgraph partial output" type="boolean">
  If enabled, streaming responses from this node will be shown in Subgraph nodes that call this graph.
</ParamField>

## Example: Simple Question Answering

1. Add an Ask AI block to your flow.
2. Add a Text block and enter your question in its editor.
3. Connect the output of the Text block to the `Prompt` input of the Ask AI block.
4. Select your desired model in the Ask AI block settings.
5. Run your flow. The AI's response will appear at the bottom of the Ask AI block.

## Error Handling

The block will retry failed attempts up to 3 times with exponential backoff:
- Minimum retry delay: 500ms
- Maximum retry delay: 5000ms
- Retry factor: 2.5x
- Includes randomization
- Maximum retry time: 5 minutes

Error messages will be logged for:
- Missing prompt input
- API rate limits (will retry)
- API timeouts (will retry) 
- Token limit exceeded
- Invalid model configuration
- Other API errors

<Warning>
  Be mindful of rate limits when using the Ask AI block, especially when [batching](/workflow/batching) requests.
</Warning>

## FAQ

<AccordionGroup>
  <Accordion title="Can I use multiple AI models in the same workflow?">
    Yes, you can use multiple Ask AI blocks with different models in the same workflow.
  </Accordion>

  <Accordion title="What happens if I connect an unsupported data type to the prompt input?">
    The block will attempt to convert the value to a string. If successful, it will be treated as a user message. If conversion fails, the input will be ignored.
  </Accordion>

  <Accordion title="Are all settings available for all models?">
    No, available settings may vary depending on the selected model. The block will adapt its settings based on your selection.
  </Accordion>
</AccordionGroup>

## See Also

- [Text Block](/block-reference/data/text)
- [Assemble Prompt Block](/block-reference/ai/assemble-prompt)
- [AI Filter Block](/block-reference/ai/ai-filter)
